{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カタカナ5文字を自動識別するモデルの構築 - 最終発表\n",
    "## 中間発表1回目まとめ\n",
    "* データセット読み込み\n",
    " - アイウエオ各200枚の画像をベクトル形式に読み込み訓練データセット、検証データセット、テストデータセットに分割\n",
    "\n",
    "\n",
    "* ニューラルネットワークのビルディングブロックとなるレイヤの定義\n",
    " - Affineレイヤ、ReLUレイヤ、Softmaxレイヤ、BatchNormalizationレイヤ\n",
    " - SGDレイヤ、RMSPropレイヤ\n",
    " - 多層MLPの定義・検証\n",
    " - 隠れ層が2層と3層の場合の比較\n",
    " - SGDとRMSPropの比較\n",
    " - エポック数調整\n",
    " - ミニバッチサイズ調整\n",
    "\n",
    "### 課題識別精度\n",
    "\n",
    "8/19時点\n",
    "\n",
    "    Test loss:0.12792950478432882 \n",
    "    Test accuracy:0.9707692307692307\n",
    "\n",
    "### 指摘事項\n",
    "\n",
    "* 純粋に正則化の効果を狙う場合、重み減衰やドロップアウトが一般的\n",
    "* 学習率を下げてみよう\n",
    "* 交差検証によって検証データ数を増やしてハイパーパラメータを決るとよい\n",
    "* 今回の3層ネットワークのように過学習になる場合は正則化しよう\n",
    "* エポック数の検証ははじめに長く計算し結果を評価、途中できれば十分 or 早期終了\n",
    "\n",
    "## 中間発表2回目まとめ\n",
    "- CNN実装\n",
    " - Convolutionレイヤ、MaxPoolingレイヤ\n",
    "- データ拡張\n",
    "- 交差検証\n",
    " - レイヤ構成、オプティマイザ（SGD/RMSProp, 学習率）、バッチサイズ\n",
    "- 課題識別精度\n",
    "- 最終発表に向けて\n",
    "\n",
    "### 課題識別精度\n",
    "\n",
    "9/1時点\n",
    "\n",
    "    Test loss:0.07289509119221133 \n",
    "    Test accuracy:0.9866153846153846 \n",
    "    \n",
    "## 最終発表\n",
    "- 実装の進捗状況\n",
    "- モデルの改良点と識別精度の変遷\n",
    "- arxiv.org等で見つけた論文とそこから得た知見\n",
    "- 課題を通しての感想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装の進捗状況\n",
    "- Global Average Pooling を実装\n",
    " - 訓練データをファイルから都度読む対応と合わせ、層を深くすることができた\n",
    "- Weight Decay と Adam, Momentum を実装\n",
    " - 汎化性能向上を見込んでWeight Decay\n",
    " - Weight Decay時にAdamは向かなかったのでMomentumを使用\n",
    "  - ドロップアウトを試しても良かったかも？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAveragePooling:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x, train_flg):\n",
    "        out = x.mean(axis=(2, 3))\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout, weight_decay_lambda):\n",
    "        n, c, h, w = self.x.shape\n",
    "        area = h * w\n",
    "        dx = dout.repeat(area).reshape(n, c, h, w)\n",
    "        dx /= area\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの改良点と識別精度の変遷\n",
    "- 中間発表１\n",
    " - 全層結合型ニューラルネットワーク\n",
    "  - Test accuracy:0.9707692307692307\n",
    "\n",
    "        Affine(784, 50) -> BatchNoralization -> ReLU -> Affine(50, 5) -> SoftmaxWithLoss\n",
    "\n",
    "- 中間発表２\n",
    " - 畳み込みニューラルネットワーク, データ拡張(+1200)\n",
    " - Test accuracy:0.9866153846153846 \n",
    "\n",
    "        Convolution(3x3x16, stride:1) -> BatchNormalization -> ReLU -> MaxPooling(4x4, stride:4) ->\n",
    "        Convolution(3x3x32, stride:1) -> BatchNormalization -> ReLU -> MaxPooling(2x2, stride:2) ->\n",
    "        Affine(512, 5) -> SoftmaxWithLoss\n",
    "        \n",
    "- 最終発表\n",
    " - 中間発表２ + データ拡張(+5000), Global Average Pooling, Weight Decay\n",
    " - Test accuracy:0.9978461538461538\n",
    "\n",
    "         Convolution(5x5x16, stride:1) -> BatchNormalization -> ReLU -> MaxPooling(2x2, stride:4) ->\n",
    "         Convolution(3x3x64, stride:1) -> BatchNormalization -> ReLU ->\n",
    "         Convolution(3x3x64, stride:1) -> BatchNormalization -> ReLU -> MaxPooling(2x2, stride:2) ->\n",
    "         GlobalAveragePooling -> BatchNormalization -> ReLU -> Affine(64, 5) -> SoftmaxWithLoss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arxiv.org等で見つけた論文とそこから得た知見\n",
    "- Global Average Pooling\n",
    " - https://arxiv.org/abs/1312.4400\n",
    " - パラメータ数を劇的に減らすことができる\n",
    " - 最後のモデルでGlobal Average Poolingを用いなかったら Memory Error になる\n",
    "- VGG風 3x3 の畳み込みを２つつなげて 5x5 と同じ範囲をカバーしつつ表現力を上げる\n",
    " - https://arxiv.org/abs/1409.1556\n",
    "- Weight DecayでAdamを使う場合は工夫が必要\n",
    " - https://arxiv.org/abs/1711.05101\n",
    " - Adamの改良まではできなかった\n",
    " - SGD, Momentumは普通に使われるらしい\n",
    "  - https://stats.stackexchange.com/questions/70101/neural-networks-weight-change-momentum-and-weight-decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 課題を通しての感想\n",
    "\n",
    "- ボーダーラインは超えることができたが、シンプルなカタカナ５文字でも99%を超えてからなかなか精度が上がらず学習の難しさを痛感した。\n",
    "- 計算機のメモリが少なめ？でパラメータ数削減の効力を身にして感じた。\n",
    "- データ拡張をどこまで行えばよいのか見当がつかなかった。適当に拡張してもモデルがそれにフィットしすぎて精度が上がらない現象（過学習）に遭遇。\n",
    " - 訓練したモデルの最良のエポック数は精度が一番良く（誤差が一番低く）なったエポックを採用しても過学習している可能性をはらまないか？検証集合を用いて評価していれば問題ないか？\n",
    " - 検証集合に対する精度が１に達してしまった場合、（正則化をかけた上で）誤差をひたすら小さくする方向でチューニングして大丈夫か？\n",
    "- 中盤はまともに交差検証を行っていたが、終盤は交差検証を行うにも半日以上かかるようになり施行回数が激減してしまった。GPUが必須と言われる理由がよくわかった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
